![Jieba Tokenizer](jieba.svg)
# @fictionking/n8n-nodes-jieba

一个基于jieba-wasm的中文分词n8n节点插件，用于在n8n自动化工作流中进行中文文本分词处理。

## 功能特点

- 支持多种分词模式：
  - 精确模式：最适合文本分析
  - 全模式：扫描出文本中所有可能的词语
  - 搜索引擎模式：在精确模式基础上，对长词再次切分
  - 词性分词模式：不仅分词，还标注每个词语的词性
- 支持自定义词典：
  - 文本输入：直接在界面中输入词典内容
  - 文件加载：从文件系统加载词典文件
- 高级词典支持：支持设置词语词频和词性标签
- 智能词典管理：词典只在配置变更时重新加载，提高执行效率
- 简单易用的界面配置
- 完整的错误处理
- 详细的输出结果

## 安装方法

### 方法一：通过n8n社区节点管理

在n8n的Web界面中，通过"设置" > "社区节点" > "安装"，搜索并安装`@fictionking/n8n-nodes-jieba`。

### 方法二：手动安装

```bash
# 在n8n安装目录下执行
npm install @fictionking/n8n-nodes-jieba

# 或者全局安装
npm install -g @fictionking/n8n-nodes-jieba
```

安装后，重启n8n服务，节点将会出现在节点面板的"转换"类别中。

## 使用示例

1. 在n8n工作流中添加"Jieba 分词器"节点
2. 配置输入文本（可以是静态文本或动态字段）
3. 选择分词模式
4. 连接其他节点处理分词结果

## 输出结果

节点执行后，输出数据将包含以下字段：

- `originalText`: 原始输入文本
- `tokens`: 分词结果数组（普通分词模式下）或词性标记数组（词性分词模式下）
- `tokenCount`: 分词数量
- `mode`: 使用的分词模式

### 词性分词模式输出格式

在词性分词模式下，`tokens`字段将包含词性标记的结果，格式为数组中的每个元素都是一个包含词语和词性的对象：

```json
[
  {"word": "张三", "flag": "nr"},
  {"word": "是", "flag": "v"},
  {"word": "北京大学", "flag": "nt"},
  {"word": "的", "flag": "u"},
  {"word": "学生", "flag": "n"}
]
```

其中：
- `word`: 分词后的词语
- `flag`: 词语的词性标签

## 自定义词典使用

### 词典格式支持

节点支持两种词典配置方式，均支持以下词条格式：

1. **基本格式**：仅包含词语
   ```
   张三
   李四
   五道口
   ```

2. **带词频格式**：词语后空格分隔词频
   ```
   张三 10
   李四 5
   五道口 15
   ```

3. **带词频和词性格式**：词语、词频、词性空格分隔（使用词性时必须提供词频）
   ```
   张三 10 nr
   李四 5 nr
   五道口 15 ns
   ```

### 使用文本输入词典

1. 在节点配置中选择"使用文本输入"作为词典来源
2. 在文本框中输入词条，每行一个词条
3. 可以使用以上三种格式的任意一种

### 使用文件词典

1. 在节点配置中选择"使用文件路径"作为词典来源
2. 输入词典文件的完整路径
3. 确保文件格式符合要求：每行一个词条，换行符分隔
4. 注意：确保n8n有权限读取指定的文件路径

## 开发说明

### 安装依赖

```bash
npm install
```

### 开发模式

```bash
npm run dev
```

### 构建项目

```bash
npm run build
```

### 运行测试

```bash
npm run test
```

## 许可证

MIT License